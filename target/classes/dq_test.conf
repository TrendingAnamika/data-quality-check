spark : {
jobApplicationName = "TEST_DQ"
}
DQ_preferences : {
Save_Bad_Records = false
DQ_Rules_File_dev=""
DQ_Rules_File_uat="/path/data/test_dq/test_dq.json"
DQ_Rules_File_prod="/path/data/test_dq/test_dq.json"
DQ_Rules_Capture_Bad_Records="/path/data/test_dq/"
Profiling_Records_dev=""
Profiling_Records_uat=""
Profiling_Records_prod=""
Historical_Records_dev=""
Historical_Records_uat=""
Historical_Records_prod=""
}
Input_data : {
log4j_path="/path/Data_Quality_Framework/Common/config/log4j.properties"
frwrk_config_file="/path/Data_Quality_Framework/Common/config/framework_1.0.conf"
spark_jar_file="/data/1/etlcm/app/Shared_Libraries/Data_Quality_Framework/Common/scripts/Data_quality_framework-2.03-jar-with-dependencies.jar"
Dataset_Type = hive
Dataset_Column_Separator = ","
Dataset_Header_Row_Flag = "true"
Dataset_Infer_Schema = "true"
Dataset_NULL_sub_Value = ""
Dataset_Charset = "UTF-8"
Dataset_Multi_Line_Flg = "false"
Dataset_Multi_Line_quote = ""
Dataset_Multi_Line_escape = ""
Dataset_Input_Query_dev=""
Dataset_Input_Query_uat="/path/data/test_dq/test_dq.sql"
Dataset_Input_Query_prod=""
Dataset_Name_dev=""
Dataset_Name_uat="test_db.test_dq"
Dataset_Name_prod = ""
Dataset_Custom_Schema=""
Failure_Threshold=50
}
Output_data : {
data_profile_attachment_names=""
hist_trnd_anlys_attachment_names="/tmp/path/mean/mean.csv,/tmp/path/freq/freq.csv"
hdfs_path_for_dq_result="/data/gcgeetlnadpsu/work/dq/DIGITAL_ENGAGEMENT_BEHAVIOR/"
}
spark_config : {
spark_conf_dev="--conf spark.ui.port=7051 --conf spark.port.maxRetries=100 --queue test_queue --conf spark.dynamicAllocation.enabled=false --conf spark.sql.broadcastTimeout=1000 --driver-cores 2 --executor-memory 5g --num-executors 15 --executor-cores 4 --conf spark.dynamicAllocation.executorIdleTimeout=600s --conf spark.sql.parquet.compression.codec=snappy --deploy-mode cluster --conf spark.yarn.maxAppAttempts=1 --driver-memory 10G --conf spark.yarn.am.memoryOverhead=4g --conf spark.default.parallelism=50 --conf spark.sql.hive.manageFilesourcePartitions=False --conf spark.kryoserializer.buffer.max=2046m --conf spark.sql.shuffle.partitions=50 --conf spark.shuffle.service.enabled=true"
spark_conf_uat="--conf spark.ui.port=7051 --conf spark.port.maxRetries=100 --queue test_queue --conf spark.dynamicAllocation.enabled=false --conf spark.sql.broadcastTimeout=1000 --driver-cores 2 --executor-memory 5g --num-executors 15 --executor-cores 4 --conf spark.dynamicAllocation.executorIdleTimeout=600s --conf spark.sql.parquet.compression.codec=snappy --deploy-mode cluster --conf spark.yarn.maxAppAttempts=1 --driver-memory 10G --conf spark.yarn.am.memoryOverhead=4g --conf spark.default.parallelism=50 --conf spark.sql.hive.manageFilesourcePartitions=False --conf spark.kryoserializer.buffer.max=2046m --conf spark.sql.shuffle.partitions=50 --conf spark.shuffle.service.enabled=true"
spark_conf_prod="--conf spark.ui.port=7051 --conf spark.port.maxRetries=100 --queue test_queue --conf spark.dynamicAllocation.enabled=false --conf spark.sql.broadcastTimeout=1000 --driver-cores 2 --executor-memory 5g --num-executors 15 --executor-cores 4 --conf spark.dynamicAllocation.executorIdleTimeout=600s --conf spark.sql.parquet.compression.codec=snappy --deploy-mode cluster --conf spark.yarn.maxAppAttempts=1 --driver-memory 10G --conf spark.yarn.am.memoryOverhead=4g --conf spark.default.parallelism=50 --conf spark.sql.hive.manageFilesourcePartitions=False --conf spark.kryoserializer.buffer.max=2046m --conf spark.sql.shuffle.partitions=50 --conf spark.shuffle.service.enabled=true"
}
err_mail_config : {
err_send_to_dev="abc.com"
err_send_to_uat="abc.com"
err_send_to_prod="abc.com"
}
dq_results_mail_config : {
dq_results_send_to_dev="abc.com"
dq_results_send_to_uat="abc.com"
dq_results_send_to_prod="abc.com"
}

